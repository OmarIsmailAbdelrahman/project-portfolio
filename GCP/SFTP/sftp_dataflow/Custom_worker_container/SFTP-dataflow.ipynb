{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2875f0-d733-41d1-b387-359989282f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "# Use the official Apache Beam SDK image as the base\n",
    "FROM apache/beam_python3.10_sdk:2.53.0\n",
    "\n",
    "RUN apt-get update && apt-get install -y\n",
    "\n",
    "RUN pip install \\\n",
    "    google-cloud-pubsub \\\n",
    "    paramiko \\\n",
    "    google-cloud-secret-manager\n",
    "\n",
    "# Download and install rclone\n",
    "RUN curl -O https://downloads.rclone.org/rclone-current-linux-amd64.zip \\\n",
    "    && unzip rclone-current-linux-amd64.zip \\\n",
    "    && cd rclone-*-linux-amd64 \\\n",
    "    && cp rclone /usr/bin/ \\\n",
    "    && chown root:root /usr/bin/rclone \\\n",
    "    && chmod 755 /usr/bin/rclone \\\n",
    "    && mkdir -p /usr/local/share/man/man1 \\\n",
    "    && cp rclone.1 /usr/local/share/man/man1/\n",
    "    # && mandb\n",
    "\n",
    "# Fix protoco and gcp-storage problem\n",
    "ENV PB_REL=\"https://github.com/protocolbuffers/protobuf/releases\"\n",
    "RUN curl -LO $PB_REL/download/v30.2/protoc-30.2-linux-x86_64.zip \\\n",
    "    && unzip protoc-30.2-linux-x86_64.zip -d /root/.local \\\n",
    "    && export PATH=\"$PATH:/root/.local/bin\" \\\n",
    "    && pip install protobuf \\\n",
    "    && pip install --upgrade google-cloud-storage \\\n",
    "    && pip install --upgrade grpcio\n",
    "\n",
    "# Set the working directory (optional, but good practice)\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy your Beam pipeline code into the container\n",
    "# COPY ./pipeline.py /app/pipeline.py\n",
    "\n",
    "\n",
    "# You might want to set a default command to run your pipeline\n",
    "ENTRYPOINT [\"/opt/apache/beam/boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f1047b2-e930-4664-bac3-ff2f1cb33a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline.py\n",
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "import tempfile\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "\n",
    "\n",
    "class RcloneMixin:\n",
    "    def _prepare_rclone(self):\n",
    "        import tempfile\n",
    "      # PROJECT, SECRET, REMOTE, BUCKET configuration\n",
    "        PROJECT       = 'omar-ismail-test-project'\n",
    "        SECRET_NAME   = 'sftp_default_key'\n",
    "        RCLONE_REMOTE = 'myvm' # you must setup the rclone for it to work\n",
    "        REMOTE_DIR    = 'customer_test_data'\n",
    "\n",
    "        HOST          = '34.69.92.10'\n",
    "        USER          = 'omar_ismail'\n",
    "        PORT          = 22\n",
    "        GCS_BUCKET    = 'new_remove'\n",
    "        from google.cloud import secretmanager\n",
    "        # 1) Fetch key from Secret Manager\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "        name   = f\"projects/{PROJECT}/secrets/{SECRET_NAME}/versions/latest\"\n",
    "        key_str = client.access_secret_version(request={\"name\": name}) \\\n",
    "                        .payload.data.decode('utf-8')\n",
    "\n",
    "        # 2) Write key to secure temp file\n",
    "        self.key_path = os.path.join(tempfile.gettempdir(), 'sftp_key')\n",
    "        print(f\"key location: {self.key_path}\")\n",
    "        with open(self.key_path, 'w') as f:\n",
    "            f.write(key_str)\n",
    "        os.chmod(self.key_path, 0o600)\n",
    "\n",
    "        # 3) Write a minimal rclone.conf\n",
    "        self.conf_path = os.path.join(tempfile.gettempdir(), 'rclone.conf')\n",
    "        conf = f\"\"\"\n",
    "[{RCLONE_REMOTE}]\n",
    "type = sftp\n",
    "host = {HOST}\n",
    "user = {USER}\n",
    "port = {PORT}\n",
    "key_file = {self.key_path}\n",
    "\"\"\"\n",
    "        with open(self.conf_path, 'w') as f:\n",
    "            f.write(conf.strip() + \"\\n\")\n",
    "        print(\"rclone configuration directory\", self.conf_path)\n",
    "\n",
    "class ListFiles(RcloneMixin, beam.DoFn):\n",
    "    def setup(self):\n",
    "        self._prepare_rclone()\n",
    "    def process(self, _):\n",
    "        import subprocess\n",
    "        RCLONE_REMOTE = 'myvm' # you must setup the rclone for it to work\n",
    "        REMOTE_DIR    = 'customer_test_data'\n",
    "        print(\"fetch list of files\")\n",
    "        out = subprocess.check_output([\n",
    "            'rclone', '--config', self.conf_path,\n",
    "            'lsf', f\"{RCLONE_REMOTE}:{REMOTE_DIR}\"\n",
    "        ]).decode('utf-8').splitlines()\n",
    "        print(\"finish fetching\", out)\n",
    "\n",
    "        for fname in out:\n",
    "            if fname.endswith('.csv'):\n",
    "                print(f\"Found csv: {fname}\")\n",
    "                yield fname\n",
    "            if fname.endswith('.parquet'):\n",
    "                print(f\"Found parquet: {fname}\")\n",
    "                yield fname\n",
    "            \n",
    "\n",
    "\n",
    "class FetchWithRclone(RcloneMixin, beam.DoFn):\n",
    "    def setup(self):\n",
    "        import tempfile, socket, logging\n",
    "\n",
    "        self._prepare_rclone()\n",
    "        self.tmpdir = tempfile.mkdtemp()\n",
    "        \n",
    "        # per-worker logger â†’ writes to <tmpdir>/<hostname>.log\n",
    "        self.worker_id = socket.gethostname()\n",
    "        self.logfile = os.path.join(self.tmpdir, f\"{self.worker_id}.log\")\n",
    "        handler = logging.FileHandler(self.logfile)\n",
    "        handler.setFormatter(logging.Formatter(\"%(message)s\"))\n",
    "        self.logger = logging.getLogger(self.worker_id)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.logger.addHandler(handler)\n",
    "\n",
    "    def process(self, filename):\n",
    "        import subprocess\n",
    "        RCLONE_REMOTE = 'myvm' # you must setup the rclone for it to work\n",
    "        REMOTE_DIR    = 'customer_test_data'\n",
    "        local_path = os.path.join(self.tmpdir, filename)\n",
    "        print(f\"the new tmp file in {local_path}\")\n",
    "        print(f\"copying from {RCLONE_REMOTE}:{REMOTE_DIR}/{filename}\")\n",
    "        subprocess.run([\n",
    "            'rclone', '--config', self.conf_path,\n",
    "            'copyto',\n",
    "            f\"{RCLONE_REMOTE}:{REMOTE_DIR}/{filename}\",\n",
    "            local_path\n",
    "        ], check=True)\n",
    "        self.logger.info(filename)\n",
    "        print(f\"finished copying {local_path}\")\n",
    "        yield local_path\n",
    "    def teardown(self):\n",
    "            # at worker shutdown you can inspect self.logfile\n",
    "            # or (if bucket_name given) upload it to GCS:\n",
    "        from google.cloud import storage\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(\"remove_later123\")\n",
    "        blob = bucket.blob(f\"dataflow_testing/{os.path.basename(self.logfile)}\")\n",
    "        blob.upload_from_filename(self.logfile)\n",
    "\n",
    "\n",
    "# class CopyToGCS(beam.DoFn):\n",
    "#     def __init__(self, target_uri):\n",
    "#         self.target_uri = target_uri\n",
    "#     def process(self, local_path):\n",
    "\n",
    "#         from apache_beam.io.filesystems import FileSystems\n",
    "#         print(f\"moving file {local_path} to {self.target_uri}{local_path.split('/')[-1]}\")\n",
    "#         FileSystems.copy([local_path], [self.target_uri+local_path.split('/')[-1]])\n",
    "#         yield local_path\n",
    "\n",
    "\n",
    "class UploadToGCS(beam.DoFn):\n",
    "    def __init__(self, bucket_name, prefix=\"\"):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.prefix = prefix.strip(\"/\")\n",
    "\n",
    "    def setup(self):\n",
    "        from google.cloud import storage\n",
    "        self.client = storage.Client()\n",
    "        self.bucket = self.client.bucket(self.bucket_name)\n",
    "\n",
    "    def process(self, local_path):\n",
    "        import os\n",
    "        filename = os.path.basename(local_path)\n",
    "        if self.prefix:\n",
    "            blob_path = f\"{self.prefix}/{filename}\"\n",
    "        else:\n",
    "            blob_path = filename\n",
    "\n",
    "        blob = self.bucket.blob(blob_path)\n",
    "        blob.upload_from_filename(local_path)\n",
    "\n",
    "        os.remove(local_path)\n",
    "        gcs_uri = f\"gs://{self.bucket_name}/{blob_path}\"\n",
    "        print(f\"moved file {local_path} to {gcs_uri}\")\n",
    "        yield gcs_uri\n",
    "\n",
    "\n",
    "\n",
    "# def parse_csv(local_path):\n",
    "#     import csv\n",
    "#     with open(local_path, newline='') as f:\n",
    "#         for row in csv.DictReader(f):\n",
    "#             yield row\n",
    "\n",
    "\n",
    "def run():\n",
    "    project_id = 'omar-ismail-test-project'\n",
    "    options = PipelineOptions(\n",
    "        # streaming=True,\n",
    "        project=project_id,\n",
    "        region='us-central1',\n",
    "        temp_location='gs://remove_later123/temp',\n",
    "        # service_account_email='dataflow-worker@omar-ismail-test-project.iam.gserviceaccount.com'\n",
    "    )\n",
    "\n",
    "    with beam.Pipeline(options=options) as p:\n",
    "        files = (\n",
    "            p\n",
    "            | 'Init'      >> beam.Create([None])\n",
    "            | 'ListFiles' >> beam.ParDo(ListFiles())\n",
    "            | 'Reshuffle' >> beam.Reshuffle()\n",
    "\n",
    "        )\n",
    "\n",
    "        fetched = files | 'FetchOne' >> beam.ParDo(FetchWithRclone())\n",
    "\n",
    "        _ = fetched | 'SaveRaw' >> beam.ParDo(\n",
    "            UploadToGCS(bucket_name=\"new_remove\", prefix=\"raw\"))\n",
    "            # note: you can parameterize per-filename if needed\n",
    "\n",
    "        # _ = fetched | 'ParseCSV' >> beam.FlatMap(parse_csv) \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd9272-527b-4463-9c7d-0c49c329107d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade apache-beam[gcp]==2.53.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b49ab441-5145-4c14-9aa2-e41fef17b54f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.16\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc71cfd7-017b-4a34-9eb2-5b6139e1daff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python pipeline.py \\\n",
    "#   --runner=DataflowRunner \\\n",
    "#   --project=omar-ismail-test-project \\\n",
    "#   --region=us-central1 \\\n",
    "#   --temp_location=gs://remove_later123/dataflow_streaming_test/tmp/ \\\n",
    "#   --staging_location=gs://remove_later123/dataflow_streaming_test/stage/ \\\n",
    "#   --num_workers=1 \\\n",
    "#   --max_num_workers=2 \\\n",
    "#   --autoscaling_algorithm=THROUGHPUT_BASED \\\n",
    "#   --worker_machine_type=e2-small \\\n",
    "#   --experiments=use_runner_v2 \\\n",
    "#   --flink_version=1.15 \\\n",
    "#   --sdk_container_image='gcr.io/omar-ismail-test-project/beam-rclone-image:latest'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a49f6b-9e93-4a2e-a480-7608c653f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rclone binaries\n",
    "# curl -O https://downloads.rclone.org/rclone-current-linux-amd64.zip\n",
    "# unzip rclone-current-linux-amd64.zip\n",
    "# cd rclone-*-linux-amd64\n",
    "\n",
    "# # move the files\n",
    "# cp rclone /usr/bin/\n",
    "# chown root:root /usr/bin/rclone\n",
    "# chmod 755 /usr/bin/rclone\n",
    "# # Install manpage\n",
    "# mkdir -p /usr/local/share/man/man1\n",
    "# cp rclone.1 /usr/local/share/man/man1/\n",
    "# # mandb ### this doesn't work for some reason, but i can access rclone now\n",
    "\n",
    "# #protoco install newer version, but didn't work correctly\n",
    "# PB_REL=\"https://github.com/protocolbuffers/protobuf/releases\"\n",
    "# curl -LO $PB_REL/download/v30.2/protoc-30.2-linux-x86_64.zip\n",
    "# unzip protoc-30.2-linux-x86_64.zip -d $HOME/.local\n",
    "# export PATH=\"$PATH:$HOME/.local/bin\"\n",
    "\n",
    "# # so i used \n",
    "# pip install protobuf==3.20.*\n",
    "\n",
    "# # for storage problem i used\n",
    "# pip install --upgrade google-cloud-storage\n",
    "\n",
    "# # for error \"import name 'aio' from 'grpc'\"\n",
    "# pip install --upgrade grpcio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a0748ad-cf10-4a12-bb7b-68a2b4836495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/tmp'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tempfile\n",
    "tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716edad1-bb10-4d69-bea2-538544b6ce83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
