{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4735ddf0-3506-41c2-9664-55d48bd9e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "# Use the official Apache Beam SDK image as the base\n",
    "FROM apache/beam_python3.10_sdk:2.53.0\n",
    "\n",
    "COPY --from=gcr.io/dataflow-templates-base/python311-template-launcher-base:20230622_RC00 /opt/google/dataflow/python_template_launcher /opt/google/dataflow/python_template_launcher\n",
    "\n",
    "RUN apt-get update && apt-get install -y\n",
    "\n",
    "# Install dependencies with specific versions\n",
    "RUN pip install --no-cache-dir \\\n",
    "    paramiko \\\n",
    "    protobuf \\\n",
    "    pyarrow \\ \n",
    "    google-cloud-storage\n",
    "\n",
    "# Download and install rclone\n",
    "# RUN curl -O https://downloads.rclone.org/rclone-current-linux-amd64.zip \\\n",
    "#     && unzip rclone-current-linux-amd64.zip \\\n",
    "#     && cd rclone-*-linux-amd64 \\\n",
    "#     && cp rclone /usr/bin/ \\\n",
    "#     && chown root:root /usr/bin/rclone \\\n",
    "#     && chmod 755 /usr/bin/rclone \\\n",
    "#     && mkdir -p /usr/local/share/man/man1 \\\n",
    "#     && cp rclone.1 /usr/local/share/man/man1/\n",
    "    # && mandb\n",
    "\n",
    "# Fix protoco and gcp-storage problem\n",
    "ENV PB_REL=\"https://github.com/protocolbuffers/protobuf/releases\"\n",
    "RUN curl -LO $PB_REL/download/v30.2/protoc-30.2-linux-x86_64.zip \\\n",
    "    && unzip protoc-30.2-linux-x86_64.zip -d /root/.local \\\n",
    "    && export PATH=\"$PATH:/root/.local/bin\" \\\n",
    "    && pip install protobuf \\\n",
    "    && pip install --upgrade google-cloud-storage \\\n",
    "    && pip install --upgrade grpcio\n",
    "\n",
    "# Set the working directory (optional, but good practice)\n",
    "\n",
    "# Copy your Beam pipeline code into the container\n",
    "ARG WORKDIR=/template\n",
    "WORKDIR ${WORKDIR}\n",
    "\n",
    "COPY main.py .\n",
    "# COPY pyproject.toml .\n",
    "# COPY requirements.txt .\n",
    "# COPY setup.py .\n",
    "COPY src src\n",
    "\n",
    "\n",
    "ENV FLEX_TEMPLATE_PYTHON_PY_FILE=\"${WORKDIR}/main.py\"\n",
    "\n",
    "\n",
    "# You might want to set a default command to run your pipeline\n",
    "ENTRYPOINT [\"/opt/apache/beam/boot\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750717f-897d-4a5a-bf83-9ccda6b0edae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tranformation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tranformation.py\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.io.parquetio import ReadFromParquet, WriteToParquet\n",
    "import typing\n",
    "import pyarrow as pa\n",
    "from apache_beam.io.filesystems import FileSystems\n",
    "import pyarrow.parquet as pq\n",
    "from apache_beam.transforms.combiners import Sample\n",
    "import json \n",
    "\n",
    "# 1. Extract column-value pairs\n",
    "def extract_object_column_values(record: dict): # this is \n",
    "    for col, val in record.items():\n",
    "        if isinstance(val, str):  # Candidate for label encoding\n",
    "            yield (col, val)\n",
    "\n",
    "\n",
    "# 2. Create label mapping\n",
    "def create_label_mapping(kv: typing.Tuple[str, typing.Iterable[str]]):\n",
    "    col, values = kv\n",
    "    unique_vals = set(values)\n",
    "    if len(unique_vals) <= 15:\n",
    "        mapping = {v: i for i, v in enumerate(sorted(unique_vals))}\n",
    "        return (col, mapping)\n",
    "    return None  # skip this column\n",
    "\n",
    "class Convert(beam.DoFn):\n",
    "    def process(self,record):\n",
    "        new_mappings = {}\n",
    "        for k,v in record.items():\n",
    "            # print(k,v)\n",
    "            new_mappings[k] = v\n",
    "        return [new_mappings]\n",
    "\n",
    "# 3. Encode\n",
    "class FilterUsingLength(beam.DoFn):\n",
    "    def process(self,record: dict, mappings: list):                \n",
    "        new_record = record.copy()\n",
    "        for col, mapping in mappings[0].items():\n",
    "            val = new_record.get(col)\n",
    "            if val in mapping:\n",
    "                new_record[col] = mapping[val]\n",
    "            elif val is not None:\n",
    "                new_record[col] = -1  # Unknown\n",
    "        yield new_record\n",
    "\n",
    "def infer_schema(parquet_path: str):\n",
    "    with FileSystems.open(parquet_path) as f:\n",
    "        return pq.ParquetFile(f).schema_arrow\n",
    "\n",
    "\n",
    "def run():\n",
    "    options = PipelineOptions()\n",
    "    with beam.Pipeline(options=options) as p:\n",
    "\n",
    "        ## because no schema is provided for the parquet file\n",
    "        schema_directory = 'gs://new_remove/test_transformation/result.parquet'\n",
    "        orig = pq.ParquetFile(FileSystems.open(schema_directory)).schema_arrow\n",
    "\n",
    "        mapped_cols = [\n",
    "            'po_priority_common_key',\n",
    "            'po_to_department_key',\n",
    "            'po_department_key',\n",
    "            'po_mode_common_key',\n",
    "            'po_type_common_key',\n",
    "            'po_comm_common_key',\n",
    "            'po_user_department_key',\n",
    "            'po_encounter_type_common_key',\n",
    "            'po_prescription_status_common_key',\n",
    "            'duration_uom_common_key',\n",
    "            'status_common_key',\n",
    "            'result_status_common_key',\n",
    "            'priority_common_key',\n",
    "            'status_flag',\n",
    "            'result_type',\n",
    "            'HLN',\n",
    "        ]\n",
    "\n",
    "\n",
    "        schema = pa.schema(\n",
    "            # first, your newly-int64 columns:\n",
    "            [pa.field(c, pa.int64()) for c in mapped_cols] +\n",
    "            # then all the other fields untouched:\n",
    "            [f for f in orig if f.name not in mapped_cols]\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Read data\n",
    "        records = p | 'Read Parquet' >> ReadFromParquet('gs://new_remove/test_transformation/result.parquet')\n",
    "\n",
    "        column_values = records | 'Extract Col-Value' >> beam.FlatMap(extract_object_column_values) # for each record check the value type, then send them as pair to match, ex: \"(ID,3)\"\n",
    "        \n",
    "        grouped = column_values | 'Group by Column' >> beam.GroupByKey() # group the values under column name, ex: ID:[3,5,1,6]\n",
    "        \n",
    "        # Create mappings for low-cardinality columns\n",
    "        raw_mappings = grouped | 'Create Mappings' >> beam.Map(create_label_mapping) # this takes the a single tuple that contain the key \"column\", and iteratable for all values in it, it takes a single column at a time because it's a map function\n",
    "        label_mappings = raw_mappings | 'Drop Nones' >> beam.Filter(lambda x: x is not None) ## this filter out the columns with no mapping \n",
    "        # label_mappings | \"print_label\" >> beam.Map(print)\n",
    "        \n",
    "        # Convert mapping to dict for side input\n",
    "        mappings_dict = label_mappings | 'To Dict' >> beam.combiners.ToDict() # Convert all the \n",
    "        # mappings_dict | 'print' >> beam.Map(print)\n",
    "        \n",
    "        mappings_dict2 = mappings_dict | 'test1' >> beam.ParDo(Convert()) \n",
    "        # mappings_dict2 | \"test2\" >> beam.Map(print)\n",
    "        \n",
    "        encoded = records | 'Encode Records' >> beam.ParDo(FilterUsingLength(),beam.pvalue.AsList(mappings_dict2))\n",
    "\n",
    "        encoded | 'Write Compressed Parquet' >> WriteToParquet(\n",
    "            file_path_prefix='gs://new_remove/test_transformation/output2/encoded',\n",
    "            schema=schema,            # use the pre-computed schema\n",
    "            codec='snappy',\n",
    "            num_shards=1,\n",
    "            shard_name_template='',\n",
    "            file_name_suffix='.parquet'\n",
    "        )\n",
    "        # encoded | beam.io.WriteToText(\n",
    "        #           'poc_samples/Data/encoded_sample',\n",
    "        #           file_name_suffix='.json',\n",
    "        #           shard_name_template=''\n",
    "        # )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659cb6f4-ae82-4ddd-8c54-a5e4d82dad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commands for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502de290-5a3e-432b-bf73-53e5e5c7e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables needed to be set\n",
    "export PROJECT=\"\"\n",
    "export BUCKET=\"new_remove\"\n",
    "export REGION=\"us-central1\"\n",
    "export TAG=\"parquet-processing\"\n",
    "export SDK_CONTAINER_IMAGE=\"gcr.io/$PROJECT/dataflow-flex-template-parquest-processing:$TAG\"\n",
    "export TEMPLATE_FILE=gs://$BUCKET/dataflow_template_configuration-$TAG.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef4142-7314-44ab-b035-e06f5a3d2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and push to image repo\n",
    "gcloud builds submit . --tag $SDK_CONTAINER_IMAGE --project $PROJECT\n",
    "# build flex-template file in bucket\n",
    "gcloud dataflow flex-template build $TEMPLATE_FILE      --image $SDK_CONTAINER_IMAGE     --sdk-language \"PYTHON\"     --metadata-file=metadata.txt     --project $PROJECT\n",
    "# run the flex-template\n",
    "gcloud dataflow flex-template run \"flex-parquet-$(date +%Y%m%d-%H%M%S)\"   --template-file-gcs-location=\"$TEMPLATE_FILE\"   --region=\"$REGION\"   --staging-location=\"gs://$BUCKET/staging\"   --parameters=sdk_container_image=$SDK_CONTAINER_IMAGE,project=$PROJECT,schema_directory=gs://$BUCKET/test_transformation/result.parquet,target_file_location=gs://$BUCKET/test_transformation/result.parquet,result_file_location=gs://$BUCKET/test_transformation/output2/encoded   --project=\"$PROJECT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399dd2f4-45a1-4b75-b489-328535342e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test run\n",
    "docker run --rm -it --entrypoint=/bin/bash $SDK_CONTAINER_IMAGE\n",
    "\n",
    "python main.py \\\n",
    "  --project= \\\n",
    "  --schema_directory=gs://$PROJECT/test_transformation/result.parquet \\\n",
    "  --target_file_location=gs://$PROJECT/test_transformation/result.parquet \\\n",
    "  --result_file_location=gs://$PROJECT/test_transformation/output2/encoded \\\n",
    "  --pipeline_temp_location=$PROJECT \\\n",
    "  --codec=snappy \\\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
